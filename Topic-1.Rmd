
##Mira Ensley-Field
 
# Discussion questions: ISL 2.4.1, 2.4.3, 2.4.6
 
## 2.4.1
 
(a) The sample size n is extremely large, and the number of predictors
 p is small.*
 
- I would expect a more **flexible approach** to be better than an inflexible approach. As we increase flexibility, the variance of our model will be increasing and we run the risk of overfitting our model, and our bias will decrease as an underlying assumption about the structure of our data becomes less fixed. The question is, which one of these two will be increasing and decreasing faster. In this case, where there are few predictors but lots of data, variance will be increasing slower than bias is decreasing With few predictors, it is probably difficult to come up with a good model assumption and it is likely there will be systematic error in that we are omitting relevant variables. Variance however, will be less affected by few predictors so increasing flexibility will be a better approach.
 
(b) The number of predictors p is extremely large, and the number
 of observations n is small.*
 
- I would expect an **inflexible approach** to work better in this case, for the opposite reasons. If we have lots of predictors, but little data the trade-off between bias and variance still exists, but we are better able to make a good underlying model assumption using our predictors, and variance will be high due to our small n. Very flexible models require large observations (n) to be good
 
(c) The relationship between the predictors and response is highly
 non-linear.
 
- I would expect a **flexible** approach to work better because it is going to be difficult to come up with an simple underlying assumption about the relationship between predictors and response, and our attempt to model that assumption will likely deviate substaintially from the "true" equation. Thus, bias will be high so we should try to use a flexible approach.

(d) The variance of the error terms, i.e. σ2 = Var(), is extremely
 high.

- I would expect an **inflexible** approach to work better, a high amount of ε (epsilon/ irreducible error/noise) tends to uncrease the variance of the test MSE, making it less reliable and predictive.

##2.4.3
*We now revisit the bias-variance decomposition.
(a) Provide a sketch of typical (squared) bias, variance, training error,
 test error, and Bayes (or irreducible) error curves, on a single
 plot, as we go from less flexible statistical learning methods
 towards more flexible approaches. The x-axis should represent
 the amount of flexibility in the method, and the y-axis should
 represent the values for each curve. There should be five curves.
 Make sure to label each one.*
 
```{r}
plot(1, xlim = c(0, 100), ylim = c(0,100), type="n", ylab= "Hi                           Low ", xlab = "Flexibility"
     , xaxt="n",yaxt="n", main="How bias, variance, training error, test error, and irreducible error respond to increasing flexibility" )

#Variance calcs:
var<-seq(from =0, to = 100, by = 5)
varX<-var
varZ<-(exp(1)^(.035*var))
lines(varX,varZ,col="blue")
text(x=100,y=40,col="blue", labels="Variance")


#Bias calcs
bias<-seq(from =0, to = 100, by = 5)
biasX<-var
biasY<-(-exp(1)^(.035*var)+40)
lines(biasX,biasY,col="red")
text(x=100,y=20,col="red", labels="Bias")

#irreducible error
irrX<-seq(from =0, to = 100, by = 100)
irrY<-c(5,5)
lines(irrX,irrY,col="black")
text(x=100,y=3,labels="irreducible error")
text(x=100,y=0,labels=" error")


#Training error
bias<-seq(from =0, to = 100, by = 5)
biasX<-bias
biasY<-(-exp(1)^(.04*bias)+70)
lines(biasX,biasY,col="purple")
text(x=70,y=60,col="purple", labels="Training Error")

#Test error
p1X<-seq(from =0, to = 30, by = 5)
p1Y<-(-p1X^(.9)+90)
lines(p1X,p1Y,col="gold")

p2X<-seq(from=30,to=100,by=5)
p2Y<-exp(1)^(.035*p2X)+66
lines(p2X,p2Y,col="gold")
text(x=80,y=100,col="gold", labels="Test Error")




```

The **Irreducible** curve is low because in this hypothetical graph I am assuming we have decent instruments that don't have a lot of noise/random error. The level of irreducible error depends on the study, and it is flat because it doesn't change no matter how flexible of a model we use. 

**Bias** starts high in the inflexible model, perhaps because it doesn't represent the "True" form particularly well, but decreases as flexibility increases. This is because as we allow the model to have fewer underlying assumptions about the structure of the data, there will be less systematic error due to differences in model structure and the "True"" model. As a general rule, bias always decreases, but depending on the data, it can decrease at very different rates.

**Variance** starts low and then increases as flexibility increases. Variance is the amount of change between the test and training data, and as the model becomes more and more flexible, it can become overfitted, essentially it follows the training data better and better, but loses its connection to the test data. The increasing variance and decreasing bias with increasing flexibility is know as the bias-variance tradeoff. Again, variance will always increase as flexibility increases, and depending on the data, that happens at different rates.

The **testing MSE** has a parabolic or "U" shape. It has a minimum at the point at which bias has decreased a lot and variance hasn't begun to rise too much, so naturally it depends on the rate at which variance and bias and increasing and decreasing respectively. This is what the ideal model should be attempting to minimize.

The **training MSE** starts high and decreases. It can never increase as flexibility increases because the model will just get more and more specific to the training data.This is essentially like adding more paramters, even adding completely unrelated explanatory variables will never make the training MSE go down and will usually make it go up a little bit. 
 
##2.4.6
 
*Describe the differences between a parametric and a non-parametric
 statistical learning approach. What are the advantages of a parametric
 approach to regression or classification (as opposed to a nonparametric
 approach)? What are its disadvantages?*
 
- Parametric models make an assumption about the "True" form of the data (for example, that it is linear) and they then fit model parameters that will minimize residual error according to some criterion (for example, Ordinary Least Squares criterion). This approach is good becase it gives a quantitative fit, it can provide useful predictions (such as extrapolating the line for a regression, making a best guess for a classification) and it is fairly easy to interpret. Most people are very familiar with an R^2 value or a p value on different coefficients. Some disadvantages are that one could always be wrong about the underling structure of the data, and one could also feel tempted to add as many possible predictors as possible to the model, even when they aren't very good descriptors, which could lead to overfitting. 
    + examples of parametric approach include...
    + linear regression and Ordinary Least Squares
 
- Nonparametric models don't assume or try to find the "True" form of the data, instead they find a pattern without this underlying assumption. They essentially try to fit a model to the data without being obligated to make an assumption about the true form of the data. This can be very useful when there is a wide range of values and/or shapes for the ideal f(x) that are difficult to create a model for. The disadvatages are that they need a lot of data, and that it is more difficult to interpret. Additionally, often non-parametric models need to balance choosing the correct amount of flexibility or smoothness which can be a problem.
    + examples of nonparametric approach...
    + thin-plate spline
 
 
##Computing assignment: ISL 2.4.8, 2.4.9

#2.4.8

a.) 
```{r}
library(ISLR)
data(College,package = "ISLR")

```

b.) I could not get the fix() function to work..I got an error message with the start data.  editor. But I have been told that is becase the fix() function is obsolete and also there are better ways to manipulate data that don't operate so directly with the data.

```{r,eval = FALSE}
fix(College)
```

c.) 
```{r}
summary(College)
```

c.) ii

```{r}
pairs(College[1:10])
```

c.) iii

```{r}
boxplot(College$Outstate~College$Private, ylab="Tuition", xlab="Private", main="Private vs Public Schools Out-of-State Tuition")
```

c.) Elite iv:  There are 78 'Elite' universities

```{r}
 
 Elite=rep("No",nrow(College))
 Elite[College$Top10perc>50]="Yes"
 Elite=as.factor(Elite)
 College=data.frame(College, Elite)
 
summary(Elite)
 
boxplot(College$Outstate~College$Elite,ylab="Tuition", xlab="Elite", main="Elite (Top 10% Schools) vs other Schools Out-of-State Tuition")
```

c.) v
```{r}
par(mfrow=c(2,2))

hist(College$Outstate, breaks=5)
hist(College$Outstate, breaks=10)
hist(College$Outstate, breaks=20)
hist(College$Outstate, breaks=40)


```


c.) Exploring data summary vi

I looked to see whether there were any immediately obvious associations between different characteristics of schools and whether they spent a lot of money per student. I calculated an "admissions rate" as an additional and more inuitive parameter, as it is one I am very used to seeing. I found a few unsurprising things, such as  characteristics such as   low admissions rates and low  student-to-faculty ratios,  tend to increase along with expenditure per student. And characteristics such as lots oh PhD professors, a high graduation rate,  also tend to be high when expenditure is high. I also notice there seems to be a number of small, very selective schools that appear to be driving a lot of the associations, and often appear as outliers. It would also be interesting to compare this data to a College Rankings list and see how well we can predict (perhaps even crack the code!) of how College Ranking works.  (I set code that I used to ```{r, include=FALSE } because this is so long already...)


```{r, include=FALSE}

College$Admiss<-College$Accept/College$Apps

hist(College$Admiss, breaks=40)
plot(Expend~Admiss,data=College)

plot(Expend~S.F.Ratio,data=College)
plot(Expend~Private,data=College)
plot(Expend~F.Undergrad,data=College)
plot(Expend~PhD,data=College)
plot(Expend~Grad.Rate,data=College)
plot(Expend~perc.alumni,data=College)

```
 
 


#2.4.9
a.)
```{r}
library(ISLR)
data(Auto,package = "ISLR")

```
 
a.)
Quantitative predictors:
- mpg

- cylinders (but could make a  case that this is categorical as well since there are only certain numbers of cylinders cars have)

- displacement

- horsepower

- weight

- acceleration

- year

- origin"      

- "name" 

Qualitative predictors:

- cylinders (arguably--see above)  

- origin

- name" 


#Theory assignment: ISL 2.4.2, 2.4.7.
 
## 2.4.2
Explain whether each scenario is a classification or regression problem,
 and indicate whether we are most interested in inference or prediction.
 Finally, provide n and p.
 
*(a) We collect a set of data on the top 500 firms in the US. For each
 firm we record profit, number of employees, industry and the
 CEO salary. We are interested in understanding which factors
 affect CEO salary.*
 
- This is a regression problem because we are looking for a quantitative result (salary) based on (mostly) quantitative inputs such as number of employees, profit, and industry. If we were trying to categorize CEO pay as "high" or "low" then we'd have a classificaiton proble, but we are instead trying to quantify it. We are more interested in inference because we want to understand why CEO salary is what it is and if it has a relationship to our predictors, we are not trying to predict it. n is the amount of observations, 400 firms, and p is the number of predictors, 3.
 
*(b) We are considering launching a new product and wish to know
 whether it will be a success or a failure. We collect data on 20
 similar products that were previously launched. For each product
 we have recorded whether it was a success or failure, price
 charged for the product, marketing budget, competition price,
and ten other variables.*
 
- This is a classification problem, because our result is qualitative (yes/no) not quantitative. We are interested in predicting, not understanding the causal mechanism, because while we are collecting and using data on predictors, our end goal is to make a yes/no prediction accurately. n is 20 and p is 13 (if we include the yes/no as a predictor--which I don't think we would, p would be 14)
 
 
*(c) We are interesting in predicting the % change in the US dollar in
 relation to the weekly changes in the world stock markets. Hence
 we collect weekly data for all of 2012. For each week we record
 the % change in the dollar, the % change in the US market,
 the % change in the British market, and the % change in the
 German market.*
 
- This is a regression problem because it is trying to find the quantitative change in percent. We are interested in prediction, not the causal mechanisms. n is 52 (because one 'observaton' is one week of data) and p is 4 

4.
 
##2.4.7 
Table wtih training data, 6 observations, 3 predictors, 1 qualitative response

a.) compute Euclidian distance between each observation and the test point (0,0,0)

```{r}
Obs1<-c(0,3,0)
Obs2<-c(2,0,0)
Obs3<-c(0,1,3)
Obs4<-c(0,1,2)
Obs5<-c(-1,0,1)
Obs6<-c(1,1,1)

print(distance1<-((Obs1[1]^2+Obs1[2]^2+Obs1[3]^2))^(1/3))
print(distance2<-((Obs2[1]^2+Obs2[2]^2+Obs2[3]^2))^(1/3))
print(distance3<-((Obs3[1]^2+Obs3[2]^2+Obs3[3]^2))^(1/3))
print(distance4<-((Obs4[1]^2+Obs4[2]^2+Obs4[3]^2))^(1/3))
print(distance5<-((Obs5[1]^2+Obs5[2]^2+Obs5[3]^2))^(1/3))
print(distance6<-((Obs6[1]^2+Obs6[2]^2+Obs6[3]^2))^(1/3))
```


b.) What is our prediction with K = 1? Why?

- prediction = green

- for k=1, the closes value in distances 1-6 to 0 is distance5 of 1.25, which is green

c.) What is our prediction with K = 3? Why?

- prediction = red

- for k=3, the three closest values are distance5, distance6, and distance2, which are green, red, and red, so we predict red

d.)  If the Bayes decision boundary in this problem is highly nonlinear,
then would we expect the best value for K to be large or
small? Why?

- If the Bayes decision boundary in this problem is highly nonlinear, I would expect a larger K value to be better than a smaller one. This reminds me of earlier questions about flexiblity and non-linear models. Non-linear models tend to be complicated and less easy to fit simple patterns to, so I suspect that hving a larger K value would make better predictions. 
