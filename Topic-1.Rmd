# Topic 1 Exercises

##Mira Ensley-Field
 
 #Discussion questions: ISL 2.4.1, 2.4.3, 2.4.6
 
 ##2.4.1
 
 *(a) The sample size n is extremely large, and the number of predictors
 p is small.*
 - I would expect a more flexible approach to be better than an inflexible approach. As we increase flexibility, the variance of our model will be increasing and we run the risk of overfitting our model, and our bias will decrease as an underlying assumption about the structure of our data becomes less fixed. The question is, which one of these two will be increasing and decreasing faster. In this case, with few predictors but lots of data, variance will increasing slower than bias is increasing.
 
 *(b) The number of predictors p is extremely large, and the number
 of observations n is small.*
 
 - I would expect an inflexible approach to work better in this case, for the opposite reasons. If we have lots of predictors, but little data, it would be easy to overfit the data, but we have many predictors we can use to
 
 (c) The relationship between the predictors and response is highly
 non-linear.
 
 -
 (d) The variance of the error terms, i.e. Ïƒ2 = Var(), is extremely
 high.
 
 ##2.4.3
 *We now revisit the bias-variance decomposition.
 (a) Provide a sketch of typical (squared) bias, variance, training error,
 test error, and Bayes (or irreducible) error curves, on a single
 plot, as we go from less flexible statistical learning methods
 towards more flexible approaches. The x-axis should represent
 the amount of flexibility in the method, and the y-axis should
 represent the values for each curve. There should be five curves.
 Make sure to label each one.
 
 (b)Explain why each of the five curves has the shape displayed in
 part (a).*
 
 ##2.4.6
 
 *Describe the differences between a parametric and a non-parametric
 statistical learning approach. What are the advantages of a parametric
 approach to regression or classification (as opposed to a nonparametric
 approach)? What are its disadvantages?*
 
 - Parametric models make an assumotion about the "True" form of the data (i.e. that it is linear) and they then fit model parameters that will minimize residual error according to some criterion (ie OLS). This approach is good becase it gives a quantitative fit, it can provide useful predictions (ie extrapolating the line for a regression, making a best guess for a classification) and it is fairly easy to interpret. Some disadvantages are that one could always be wrong about the underling structure of the data, and one could also feel tempted to add as many possible predictors as possible to the model, even when they aren't very good descriptors, which could lead to overfitting.
 
 - Nonparametric models don't assume or try to find the "True" form of the data, instead they find a pattern without this underlying assumption. The disadvatages are that they need a lot of data, and that it is more difficult to interpret. Not having an underlying assumption about structure might 
 
 
#Computing assignment: ISL 2.4.8, 2.4.9
##a.)
```{r}
#download.file("http://www-bcf.usc.edu/~gareth/ISL/College.csv",destfile="College.csv")
college<-read.csv("College.csv")
```
##b.)
```{r}
 rownames(college)=college[,1]
 #fix(college)
 
 college=college[,-1]
 #fix(college)
```
##c.)
```{r}
# print(summary(college))
 
# print(pairs(college[,1:10] ))
 
 #plot(college$Outstate,college$Private)
 
 #length = 777, but saying they vary
 
 Elite=rep("No",nrow(college))
 Elite[college$Top10perc>50]="Yes"
 Elite=as.factor(Elite)
 college=data.frame(college, Elite)
 
 print(summary(Elite))
 #there are 78 'Elite' universities
 
# plot(Outstate,Private,data=Elite)

```
 
 
 
#As an example: a solution to 2.4.10.
 
#Theory assignment: ISL 2.4.2, 2.4.7.
 
 ## 2.4.2
  Explain whether each scenario is a classification or regression problem,
 and indicate whether we are most interested in inference or prediction.
 Finally, provide n and p.
 
 *(a) We collect a set of data on the top 500 firms in the US. For each
 firm we record profit, number of employees, industry and the
 CEO salary. We are interested in understanding which factors
 affect CEO salary.*
 
 - This is a regression problem because we are looking for a quantitative result (salary) based on (mostly) quantitative inputs such as number of employees, profit, and industry. If we were trying to categorize CEO pay as "high" or "low" then we'd have a classificaiton proble, but we are instead trying to quantify it. We are more interested in inference because we want to understand why CEO salary is what it is and if it has a relationship to our predictors, we are not trying to predict it. n is the amount of training data, 400 firms, and p is the number of predictors, 3.
 
 *(b) We are considering launching a new product and wish to know
 whether it will be a success or a failure. We collect data on 20
 similar products that were previously launched. For each product
 we have recorded whether it was a success or failure, price
 charged for the product, marketing budget, competition price,
and ten other variables.*
 
 This is a classification problem, because our result is qualitative (yes/no) not quantitative. We are interested in predicting, not understanding the causal mechanism, because while we are collecting and using data on predictors, our rend goal is to make a yes/no prediction accurately. n is 20 and p is 13.
 
 
 *(c) We are interesting in predicting the % change in the US dollar in
 relation to the weekly changes in the world stock markets. Hence
 we collect weekly data for all of 2012. For each week we record
 the % change in the dollar, the % change in the US market,
 the % change in the British market, and the % change in the
 German market.*
 
 - This is a regression problem because it is trying to find the quantitative change in percent. We are interested in prediction, not the causal mechanisms. n is 52, and p is 4.
 
 ##2.4.7

