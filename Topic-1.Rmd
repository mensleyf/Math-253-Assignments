
##Mira Ensley-Field
 
# Discussion questions: ISL 2.4.1, 2.4.3, 2.4.6
 
## 2.4.1
 
(a) The sample size n is extremely large, and the number of predictors
 p is small.*
 
- I would expect a more **flexible approach** to be better than an inflexible approach. As we increase flexibility, the variance of our model will be increasing and we run the risk of overfitting our model, and our bias will decrease as an underlying assumption about the structure of our data becomes less fixed. The question is, which one of these two will be increasing and decreasing faster. In this case, I suspect that on average with few predictors but lots of data, variance will increasing slower than bias is increasing. With few predictors, it is probably difficult to come up with a good model assumption and it is likely there will be systematic error in that we are omitting relevant variables. Variance however, will be less affected by few predictprs and decreased by the large n
 
(b) The number of predictors p is extremely large, and the number
 of observations n is small.*
 
- I would expect an **inflexible approach** to work better in this case, for the opposite reasons. If we have lots of predictors, but little data the trade-off between bias and variance still exists, but we are better able to make a good underlying model assumption using our predictors, and variance will be high due to our large n.
 
(c) The relationship between the predictors and response is highly
 non-linear.
 
- I would expect a **flexible** approach to work better because it is going to be difficult to come up with an simple underlying assumption about the relationship between predictors and response, and our attempt to model that assumption will likely deviate substaintially from the "true" equation. Thus, bias will be high so we should try to use a flexible approach.

(d) The variance of the error terms, i.e. σ2 = Var(), is extremely
 high.

- I would expect an **inflexible** approach to work better, a high amount of ε (epsilon/ irreducible error/noise) tends to uncrease the variance of the test MSE, making it less reliable and predictive.

##2.4.3
*We now revisit the bias-variance decomposition.
(a) Provide a sketch of typical (squared) bias, variance, training error,
 test error, and Bayes (or irreducible) error curves, on a single
 plot, as we go from less flexible statistical learning methods
 towards more flexible approaches. The x-axis should represent
 the amount of flexibility in the method, and the y-axis should
 represent the values for each curve. There should be five curves.
 Make sure to label each one.*
 
```{r}
plot(1, xlim = c(0, 100), ylim = c(0,100), type="n", ylab= "Hi                           Low ", xlab = "Flexibility"
     , xaxt="n",yaxt="n", main="How bias, variance, training error, test error, and irreducible error respond to increasing flexibility" )

#Variance calcs:
var<-seq(from =0, to = 100, by = 5)
varX<-var
varZ<-(exp(1)^(.035*var))
lines(varX,varZ,col="blue")
text(x=100,y=40,col="blue", labels="Variance")


#Bias calcs
bias<-seq(from =0, to = 100, by = 5)
biasX<-var
biasY<-(-exp(1)^(.035*var)+40)
lines(biasX,biasY,col="red")
text(x=100,y=20,col="red", labels="Bias")

#irreducible error
irrX<-seq(from =0, to = 100, by = 100)
irrY<-c(5,5)
lines(irrX,irrY,col="black")
text(x=100,y=3,labels="irreducible error")
text(x=100,y=0,labels=" error")


#Training error
bias<-seq(from =0, to = 100, by = 5)
biasX<-bias
biasY<-(-exp(1)^(.04*bias)+70)
lines(biasX,biasY,col="purple")
text(x=70,y=60,col="purple", labels="Training Error")

#Test error
p1X<-seq(from =0, to = 30, by = 5)
p1Y<-(-p1X^(.9)+90)
lines(p1X,p1Y,col="gold")

p2X<-seq(from=30,to=100,by=5)
p2Y<-exp(1)^(.035*p2X)+66
lines(p2X,p2Y,col="gold")
text(x=80,y=100,col="gold", labels="Test Error")




```

**Irreducible** curve is low because in this hypothetical we have decent instruments that don't have a lot of noise, it doesn't change no matter how flexible of a model we use. 

**Bias** starts high in the inflexible model, perhaps because it doesnät represent the äTrueÄ form particularly well, but decreases as flexibilitz increases. This is because as we allow the model to have fewer underlzing assumptions about the structure of the data, there will be less szstematic error due to differences in model structure and the Ätrue model.

**Variance** starts low and then increases as flexibilitz increases. Variance is the amount of change between the test and training data, and as the model becomes more and more flexible, it can become overfitted, essentiallz it follows the training data better and better, but loses its connection to the test data. The increasing variance and decreasing bias with increasing flexibilitz is know as the biasßvariance tradeoff.

The **testing MSE** has a parabolic shape. It has a minimum at the point at which bias has decreased and variance hasnät begun to rise to much, so naturallz it depends on the rate at which variance and bias and increasing and decreasing respectivelz. This is what the ideal model should be attempting to minimize.

The **training model** starts high and decreases. It can never increase as flexibilitz increases, this is essentiallz like adding more paramters, even adding completelz unrelated explanatorz variables will never make the training MSE go down )it will however, negativelz impact the test MSE=
 
##2.4.6
 
*Describe the differences between a parametric and a non-parametric
 statistical learning approach. What are the advantages of a parametric
 approach to regression or classification (as opposed to a nonparametric
 approach)? What are its disadvantages?*
 
- Parametric models make an assumption about the "True" form of the data (i.e. that it is linear) and they then fit model parameters that will minimize residual error according to some criterion (ie OLS). This approach is good becase it gives a quantitative fit, it can provide useful predictions (such as extrapolating the line for a regression, making a best guess for a classification) and it is fairly easy to interpret. Most people are very familiar with an R^2 value or a p value which Some disadvantages are that one could always be wrong about the underling structure of the data, and one could also feel tempted to add as many possible predictors as possible to the model, even when they aren't very good descriptors, which could lead to overfitting.
    + examples of parametric approach include...
    + linear regression and Ordinary Least Squares
 
- Nonparametric models don't assume or try to find the "True" form of the data, instead they find a pattern without this underlying assumption. This can ve very useful when there is a wide range of values and/or shapes for the ideal f(x) that are difficult to create a model for. The disadvatages are that they need a lot of data, and that it is more difficult to interpret. Not having an underlying assumption about structure might
    + examples of nonparametric approach...
    + thin-plate spline
 
 
##Computing assignment: ISL 2.4.8, 2.4.9

#2.4.8

a.) Using the method we learned in class, not book method
```{r}
library(ISLR)
data(College,package = "ISLR")

```

b.) I could not get the fix() function to work..I got an error message with the start data editor, instead I followed the example provided on the website. I still viewed the da
```{r,eval=FALSE}
fix(college)
```


b.)
```{r, eval =FALSE}
#View(College)
rownames(college)=college[,1]
fix(college)

```


```{r, eval =FALSE}
college=college[,-1]
fix(college)

```

c.) Plots (i-iii)
```{r,eval=FALSE}
summary(college)
 
pairs(college[,1:10] )

boxplot(college$Outstate,college$Private,data=college)
```
c.) Elite (iv-v)
```{}
 #plot(college$Outstate,college$Private)
 
 #length = 777, but saying they vary
 
 Elite=rep("No",nrow(college))
 Elite[college$Top10perc>50]="Yes"
 Elite=as.factor(Elite)
 college=data.frame(college, Elite)
 
 print(summary(Elite))
 #there are 78 'Elite' universities
 
# plot(Outstate,Private,data=Elite)

```
 c.) Exploring data summary (vi)
 
 
 


#2.4.9
a.)
```{r}
library(ISLR)
data(Auto,package = "ISLR")

```
 
a.)
Quantitative predictors:
- mpg
- cylinders (but could make a  case that this is categorical as well since there are only certain numbers of cylinders cars have)
- displacement
- horsepower
- weight
- acceleration
- year
- origin"       "name" 

Qualitative predictors:
- cylinders"    "displacement" "horsepower"   "weight"      
[6] "acceleration" "year"         "origin"       "name" 


#Theory assignment: ISL 2.4.2, 2.4.7.
 
## 2.4.2
Explain whether each scenario is a classification or regression problem,
 and indicate whether we are most interested in inference or prediction.
 Finally, provide n and p.
 
*(a) We collect a set of data on the top 500 firms in the US. For each
 firm we record profit, number of employees, industry and the
 CEO salary. We are interested in understanding which factors
 affect CEO salary.*
 
- This is a regression problem because we are looking for a quantitative result (salary) based on (mostly) quantitative inputs such as number of employees, profit, and industry. If we were trying to categorize CEO pay as "high" or "low" then we'd have a classificaiton proble, but we are instead trying to quantify it. We are more interested in inference because we want to understand why CEO salary is what it is and if it has a relationship to our predictors, we are not trying to predict it. n is the amount of training data, 400 firms, and p is the number of predictors, 3.
 
*(b) We are considering launching a new product and wish to know
 whether it will be a success or a failure. We collect data on 20
 similar products that were previously launched. For each product
 we have recorded whether it was a success or failure, price
 charged for the product, marketing budget, competition price,
and ten other variables.*
 
This is a classification problem, because our result is qualitative (yes/no) not quantitative. We are interested in predicting, not understanding the causal mechanism, because while we are collecting and using data on predictors, our rend goal is to make a yes/no prediction accurately. n is 20 and p is 13.
 
 
*(c) We are interesting in predicting the % change in the US dollar in
 relation to the weekly changes in the world stock markets. Hence
 we collect weekly data for all of 2012. For each week we record
 the % change in the dollar, the % change in the US market,
 the % change in the British market, and the % change in the
 German market.*
 
- This is a regression problem because it is trying to find the quantitative change in percent. We are interested in prediction, not the causal mechanisms. n is 52, and p is 

4.
 
##2.4.7 
Table wtih training data, 6 observations, 3 predictors, 1 qualitative response

a.) compute Euclidiant distance between each observation and the test point (0,0,0)

```{r}
Obs1<-c(0,3,0)
Obs2<-c(2,0,0)
Obs3<-c(0,1,3)
Obs4<-c(0,1,2)
Obs5<-c(-1,0,1)
Obs6<-c(1,1,1)

distance1<-((Obs1[1]^2+Obs1[2]^2+Obs1[3]^2))^(1/3)
distance2<-((Obs2[1]^2+Obs2[2]^2+Obs2[3]^2))^(1/3)
distance3<-((Obs3[1]^2+Obs3[2]^2+Obs3[3]^2))^(1/3)
distance4<-((Obs4[1]^2+Obs4[2]^2+Obs4[3]^2))^(1/3)
distance5<-((Obs5[1]^2+Obs5[2]^2+Obs5[3]^2))^(1/3)
distance6<-((Obs6[1]^2+Obs6[2]^2+Obs6[3]^2))^(1/3)
```

b.) Prediction if k=1
- prediction = green
- for k=1, the closes value in distances 1-6 to 0 is distance5 of 1.25, which is green

c.) Prediction if k=3
- prediction = red
- for k=3, the three closest values are distance5, distance6, and distance2, which are green, red, and red, so we predict red

d.) if Bayes decision boundary is highly non-linear, the would we expect best value of K to be large or small?

```{r}
kis3<-distance5+distance6+distance2

```

should be able to make this into a loop and save a negligible amount of time and space

myList = list(Obs1,Obs2,Obs3,Obs4,Obs5,Obs6)
distanceList = list()
count<-0
```


for (name in names(myList)) 
    print(name)
    print(myList[[name]])

```



for (name in names(myList)) {
    distance<-((name[1]^2+name[2]^2+name[3]^2))^(1/3)
    count+count+1
    distanceList[count]=distance
    print(distance)
}
print(distanceList)
```
