
##Mira Ensley-Field
 
# Discussion questions: ISL 2.4.1, 2.4.3, 2.4.6
 
## 2.4.1
 
(a) The sample size n is extremely large, and the number of predictors
 p is small.*
 
- I would expect a more **flexible approach** to be better than an inflexible approach. As we increase flexibility, the variance of our model will be increasing and we run the risk of overfitting our model, and our bias will decrease as an underlying assumption about the structure of our data becomes less fixed. The question is, which one of these two will be increasing and decreasing faster. In this case, I suspect that on average with few predictors but lots of data, variance will increasing slower than bias is increasing. With few predictors, it is probably difficult to come up with a good model assumption and it is likely there will be systematic error in that we are omitting relevant variables. Variance however, will be less affected by few predictprs and decreased by the large n
 
(b) The number of predictors p is extremely large, and the number
 of observations n is small.*
 
- I would expect an **inflexible approach** to work better in this case, for the opposite reasons. If we have lots of predictors, but little data the trade-off between bias and variance still exists, but we are better able to make a good underlying model assumption using our predictors, and variance will be high due to our large n.
 
(c) The relationship between the predictors and response is highly
 non-linear.
 
- I would expect a **flexible** approach to work better because it is going to be difficult to come up with an simple underlying assumption about the relationship between predictors and response, and our attempt to model that assumption will likely deviate substaintially from the "true" equation. Thus, bias will be high so we should try to use a flexible approach.

(d) The variance of the error terms, i.e. σ2 = Var(), is extremely
 high.

- I would expect an **inflexible** approach to work better, a high amount of ε (epsilon/ irreducible error/noise) tends to uncrease the variance of the test MSE, making it less reliable and predictive.

##2.4.3
*We now revisit the bias-variance decomposition.
(a) Provide a sketch of typical (squared) bias, variance, training error,
 test error, and Bayes (or irreducible) error curves, on a single
 plot, as we go from less flexible statistical learning methods
 towards more flexible approaches. The x-axis should represent
 the amount of flexibility in the method, and the y-axis should
 represent the values for each curve. There should be five curves.
 Make sure to label each one.*
 
 
 
 
*(b)Explain why each of the five curves has the shape displayed in
 part (a).*
 
##2.4.6
 
*Describe the differences between a parametric and a non-parametric
 statistical learning approach. What are the advantages of a parametric
 approach to regression or classification (as opposed to a nonparametric
 approach)? What are its disadvantages?*
 
- Parametric models make an assumption about the "True" form of the data (i.e. that it is linear) and they then fit model parameters that will minimize residual error according to some criterion (ie OLS). This approach is good becase it gives a quantitative fit, it can provide useful predictions (such as extrapolating the line for a regression, making a best guess for a classification) and it is fairly easy to interpret. Most people are very familiar with an R^2 value or a p value which Some disadvantages are that one could always be wrong about the underling structure of the data, and one could also feel tempted to add as many possible predictors as possible to the model, even when they aren't very good descriptors, which could lead to overfitting.
    + examples of parametric approach include...
    + linear regression and Ordinary Least Squares
 
- Nonparametric models don't assume or try to find the "True" form of the data, instead they find a pattern without this underlying assumption. This can ve very useful when there is a wide range of values and/or shapes for the ideal f(x) that are difficult to create a model for. The disadvatages are that they need a lot of data, and that it is more difficult to interpret. Not having an underlying assumption about structure might
    + examples of nonparametric approach...
    + thin-plate spline
 
 
##Computing assignment: ISL 2.4.8, 2.4.9

#2.4.8

a.) Using the method we learned in class, not book method
```{r}
library(ISLR)
data(College,package = "ISLR")

```

a.) Using the book method
```{r,eval=FALSE}
college=read.csv("College.csv", header=T,na.strings="?")
```


b.)
```{r, eval =FALSE}
#View(College)
rownames(college)=college[,1]
fix(college)

```


```{r, eval =FALSE}
college=college[,-1]
fix(college)

```

c.) Plots (i-iii)
```{r}
summary(college)
 
pairs(college[,1:10] )

boxplot(college$Outstate,college$Private,data=college)
```
c.) Elite (iv-v)
```{}
 #plot(college$Outstate,college$Private)
 
 #length = 777, but saying they vary
 
 Elite=rep("No",nrow(college))
 Elite[college$Top10perc>50]="Yes"
 Elite=as.factor(Elite)
 college=data.frame(college, Elite)
 
 print(summary(Elite))
 #there are 78 'Elite' universities
 
# plot(Outstate,Private,data=Elite)

```
 c.) Exploring data summary (vi)
 
 
 


#2.4.9
a.)
```{r}
library(ISLR)
data(Auto,package = "ISLR")

```
 
a.)
Quantitative predictors:
- mpg
- cylinders (but could make a  case that this is categorical as well since there are only certain numbers of cylinders cars have)
- displacement
- horsepower
- weight
- acceleration
- year
- origin"       "name" 
Qualitative predictors:
- cylinders"    "displacement" "horsepower"   "weight"      
[6] "acceleration" "year"         "origin"       "name" 

#As an example: a solution to 2.4.10.
 
#Theory assignment: ISL 2.4.2, 2.4.7.
 
## 2.4.2
Explain whether each scenario is a classification or regression problem,
 and indicate whether we are most interested in inference or prediction.
 Finally, provide n and p.
 
*(a) We collect a set of data on the top 500 firms in the US. For each
 firm we record profit, number of employees, industry and the
 CEO salary. We are interested in understanding which factors
 affect CEO salary.*
 
- This is a regression problem because we are looking for a quantitative result (salary) based on (mostly) quantitative inputs such as number of employees, profit, and industry. If we were trying to categorize CEO pay as "high" or "low" then we'd have a classificaiton proble, but we are instead trying to quantify it. We are more interested in inference because we want to understand why CEO salary is what it is and if it has a relationship to our predictors, we are not trying to predict it. n is the amount of training data, 400 firms, and p is the number of predictors, 3.
 
*(b) We are considering launching a new product and wish to know
 whether it will be a success or a failure. We collect data on 20
 similar products that were previously launched. For each product
 we have recorded whether it was a success or failure, price
 charged for the product, marketing budget, competition price,
and ten other variables.*
 
This is a classification problem, because our result is qualitative (yes/no) not quantitative. We are interested in predicting, not understanding the causal mechanism, because while we are collecting and using data on predictors, our rend goal is to make a yes/no prediction accurately. n is 20 and p is 13.
 
 
*(c) We are interesting in predicting the % change in the US dollar in
 relation to the weekly changes in the world stock markets. Hence
 we collect weekly data for all of 2012. For each week we record
 the % change in the dollar, the % change in the US market,
 the % change in the British market, and the % change in the
 German market.*
 
- This is a regression problem because it is trying to find the quantitative change in percent. We are interested in prediction, not the causal mechanisms. n is 52, and p is 

4.
 
##2.4.7
